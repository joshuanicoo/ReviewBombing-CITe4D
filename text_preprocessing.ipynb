{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ce7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Define directories\n",
    "cleaned_data_dir = \"./Cleaned Data\"\n",
    "sentiment_dir = \"./Sentiment Analysis CSV\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [cleaned_data_dir, sentiment_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d71ec",
   "metadata": {},
   "source": [
    "### 1.0. CSV's to Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b65a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Kingmaker:\n",
      "imdb_the_kingmaker: (23, 5)\n",
      "ltbx_earliest_the_kingmaker: (816, 8)\n",
      "ltbx_latest_the_kingmaker: (636, 8)\n",
      "rt_the_kingmaker: (73, 7)\n",
      "ltbx_the_kingmaker: (1452, 8)\n",
      "\n",
      "Praybeyt Benjamin 2:\n",
      "imdb_praybeyt_benjamin_2: (6, 5)\n",
      "ltbx_earliest_praybeyt_benjamin_2: (144, 8)\n",
      "ltbx_latest_praybeyt_benjamin_2: (144, 8)\n",
      "ltbx_praybeyt_benjamin_2: (288, 8)\n",
      "\n",
      "Hello Love Goodbye:\n",
      "imdb_hello_love_goodbye: (44, 5)\n",
      "ltbx_earliest_hello_love_goodbye: (1548, 8)\n",
      "ltbx_latest_hello_love_goodbye: (1548, 8)\n",
      "rt_hello_love_goodbye: (10, 7)\n",
      "ltbx_hello_love_goodbye: (3096, 8)\n",
      "\n",
      "Hayop Ka:\n",
      "ltbx_earliest_hayop_ka: (456, 8)\n",
      "ltbx_latest_hayop_ka: (456, 8)\n",
      "rt_hayop_ka: (8, 7)\n",
      "ltbx_hayop_ka: (912, 8)\n",
      "\n",
      "Sunshine:\n",
      "imdb_sunshine: (15, 5)\n",
      "ltbx_earliest_sunshine: (3072, 8)\n",
      "ltbx_latest_sunshine: (3072, 8)\n",
      "ltbx_sunshine: (6144, 8)\n",
      "\n",
      "Quezon:\n",
      "imdb_quezon: (8, 5)\n",
      "ltbx_earliest_quezon: (1404, 8)\n",
      "ltbx_latest_quezon: (1404, 8)\n",
      "rt_quezon: (3, 9)\n",
      "ltbx_quezon: (2808, 8)\n",
      "\n",
      "Praybeyt Benjamin 1:\n",
      "ltbx_earliest_praybeyt_benjamin_1: (2400, 8)\n",
      "ltbx_latest_praybeyt_benjamin_1: (2400, 8)\n",
      "rt_praybeyt_benjamin_1: (19, 7)\n",
      "ltbx_praybeyt_benjamin_1: (4800, 8)\n",
      "\n",
      "Mallari:\n",
      "imdb_mallari: (18, 5)\n",
      "ltbx_earliest_mallari: (3000, 8)\n",
      "ltbx_latest_mallari: (3000, 8)\n",
      "rt_mallari: (9, 9)\n",
      "ltbx_mallari: (6000, 8)\n",
      "\n",
      "Maid in Malacañang:\n",
      "imdb_maid_in_malacanang: (1339, 5)\n",
      "ltbx_earliest_maid_in_malacanang: (336, 8)\n",
      "ltbx_latest_maid_in_malacanang: (336, 8)\n",
      "ltbx_maid_in_malacanang: (672, 8)\n",
      "\n",
      "Hows of Us:\n",
      "imdb_hows_of_us: (58, 5)\n",
      "ltbx_earliest_hows_of_us: (2220, 8)\n",
      "ltbx_latest_hows_of_us: (2220, 8)\n",
      "rt_hows_of_us: (8, 7)\n",
      "ltbx_hows_of_us: (4440, 8)\n",
      "\n",
      "Felix Manalo:\n",
      "ltbx_earliest_felix_manalo: (24, 8)\n",
      "ltbx_latest_felix_manalo: (40, 8)\n",
      "rt_felix_manalo: (3, 9)\n",
      "ltbx_felix_manalo: (64, 8)\n"
     ]
    }
   ],
   "source": [
    "# The Kingmaker - Complete\n",
    "imdb_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/imdb_the_kingmaker.csv\")\n",
    "ltbx_earliest_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/ltbx_earliest_the_kingmaker.csv\")\n",
    "ltbx_latest_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/ltbx_latest_the_kingmaker.csv\")\n",
    "rt_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/rt_the_kingmaker.csv\")\n",
    "\n",
    "ltbx_the_kingmaker = pd.concat([ltbx_earliest_the_kingmaker, ltbx_latest_the_kingmaker])\n",
    "\n",
    "print(\"The Kingmaker:\")\n",
    "print(f\"imdb_the_kingmaker: {imdb_the_kingmaker.shape}\")\n",
    "print(f\"ltbx_earliest_the_kingmaker: {ltbx_earliest_the_kingmaker.shape}\")\n",
    "print(f\"ltbx_latest_the_kingmaker: {ltbx_latest_the_kingmaker.shape}\")\n",
    "print(f\"rt_the_kingmaker: {rt_the_kingmaker.shape}\")\n",
    "print(f\"ltbx_the_kingmaker: {ltbx_the_kingmaker.shape}\")\n",
    "\n",
    "# Praybeyt Benjamin 2 - No RT\n",
    "imdb_praybeyt_benjamin_2 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 2/imdb_praybeyt_benjamin_2.csv\")\n",
    "ltbx_earliest_praybeyt_benjamin_2 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 2/ltbx_earliest_praybeyt_benjamin_2.csv\")\n",
    "ltbx_latest_praybeyt_benjamin_2 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 2/ltbx_latest_praybeyt_benjamin_2.csv\")\n",
    "\n",
    "ltbx_praybeyt_benjamin_2 = pd.concat([ltbx_earliest_praybeyt_benjamin_2, ltbx_latest_praybeyt_benjamin_2])\n",
    "\n",
    "print(\"\\nPraybeyt Benjamin 2:\")\n",
    "print(f\"imdb_praybeyt_benjamin_2: {imdb_praybeyt_benjamin_2.shape}\")\n",
    "print(f\"ltbx_earliest_praybeyt_benjamin_2: {ltbx_earliest_praybeyt_benjamin_2.shape}\")\n",
    "print(f\"ltbx_latest_praybeyt_benjamin_2: {ltbx_latest_praybeyt_benjamin_2.shape}\")\n",
    "print(f\"ltbx_praybeyt_benjamin_2: {ltbx_praybeyt_benjamin_2.shape}\")\n",
    "\n",
    "# Hello Love Goodbye - Complete\n",
    "imdb_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/imdb_hello_love_goodbye.csv\")\n",
    "ltbx_earliest_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/ltbx_earliest_hello_love_goodbye.csv\")\n",
    "ltbx_latest_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/ltbx_latest_hello_love_goodbye.csv\")\n",
    "rt_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/rt_hello_love_goodbye.csv\")\n",
    "\n",
    "ltbx_hello_love_goodbye = pd.concat([ltbx_earliest_hello_love_goodbye, ltbx_latest_hello_love_goodbye])\n",
    "\n",
    "print(\"\\nHello Love Goodbye:\")\n",
    "print(f\"imdb_hello_love_goodbye: {imdb_hello_love_goodbye.shape}\")\n",
    "print(f\"ltbx_earliest_hello_love_goodbye: {ltbx_earliest_hello_love_goodbye.shape}\")\n",
    "print(f\"ltbx_latest_hello_love_goodbye: {ltbx_latest_hello_love_goodbye.shape}\")\n",
    "print(f\"rt_hello_love_goodbye: {rt_hello_love_goodbye.shape}\")\n",
    "print(f\"ltbx_hello_love_goodbye: {ltbx_hello_love_goodbye.shape}\")\n",
    "\n",
    "# Hayop Ka - Complete\n",
    "imdb_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/imdb_hayop_ka.csv\")\n",
    "ltbx_earliest_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/ltbx_earliest_hayop_ka.csv\")\n",
    "ltbx_latest_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/ltbx_latest_hayop_ka.csv\")\n",
    "rt_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/rt_hayop_ka.csv\")\n",
    "\n",
    "ltbx_hayop_ka = pd.concat([ltbx_earliest_hayop_ka, ltbx_latest_hayop_ka])\n",
    "\n",
    "print(\"\\nHayop Ka:\")\n",
    "print(f\"ltbx_earliest_hayop_ka: {ltbx_earliest_hayop_ka.shape}\")\n",
    "print(f\"ltbx_latest_hayop_ka: {ltbx_latest_hayop_ka.shape}\")\n",
    "print(f\"rt_hayop_ka: {rt_hayop_ka.shape}\")\n",
    "print(f\"ltbx_hayop_ka: {ltbx_hayop_ka.shape}\")\n",
    "\n",
    "# Sunshine - No RT\n",
    "imdb_sunshine = pd.read_csv(\"./Unclean Data/Sunshine/imdb_sunshine.csv\")\n",
    "ltbx_earliest_sunshine = pd.read_csv(\"./Unclean Data/Sunshine/ltbx_earliest_sunshine.csv\")\n",
    "ltbx_latest_sunshine = pd.read_csv(\"./Unclean Data/Sunshine/ltbx_latest_sunshine.csv\")\n",
    "\n",
    "ltbx_sunshine = pd.concat([ltbx_earliest_sunshine, ltbx_latest_sunshine])\n",
    "\n",
    "print(\"\\nSunshine:\")\n",
    "print(f\"imdb_sunshine: {imdb_sunshine.shape}\")\n",
    "print(f\"ltbx_earliest_sunshine: {ltbx_earliest_sunshine.shape}\")\n",
    "print(f\"ltbx_latest_sunshine: {ltbx_latest_sunshine.shape}\")\n",
    "print(f\"ltbx_sunshine: {ltbx_sunshine.shape}\")\n",
    "\n",
    "# Quezon - Complete\n",
    "imdb_quezon = pd.read_csv(\"./Unclean Data/Quezon/imdb_quezon.csv\")\n",
    "ltbx_earliest_quezon = pd.read_csv(\"./Unclean Data/Quezon/ltbx_earliest_quezon.csv\")\n",
    "ltbx_latest_quezon = pd.read_csv(\"./Unclean Data/Quezon/ltbx_latest_quezon.csv\")\n",
    "rt_quezon = pd.read_csv(\"./Unclean Data/Quezon/rt_quezon.csv\")\n",
    "\n",
    "ltbx_quezon = pd.concat([ltbx_earliest_quezon, ltbx_latest_quezon])\n",
    "\n",
    "print(\"\\nQuezon:\")\n",
    "print(f\"imdb_quezon: {imdb_quezon.shape}\")\n",
    "print(f\"ltbx_earliest_quezon: {ltbx_earliest_quezon.shape}\")\n",
    "print(f\"ltbx_latest_quezon: {ltbx_latest_quezon.shape}\")\n",
    "print(f\"rt_quezon: {rt_quezon.shape}\")\n",
    "print(f\"ltbx_quezon: {ltbx_quezon.shape}\")\n",
    "\n",
    "# Praybeyt Benjamin 1 - Complete\n",
    "imdb_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/imdb_praybeyt_benjamin_1.csv\")\n",
    "ltbx_earliest_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/ltbx_earliest_praybeyt_benjamin_1.csv\")\n",
    "ltbx_latest_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/ltbx_latest_praybeyt_benjamin_1.csv\")\n",
    "rt_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/rt_praybeyt_benjamin_1.csv\")\n",
    "\n",
    "ltbx_praybeyt_benjamin_1 = pd.concat([ltbx_earliest_praybeyt_benjamin_1, ltbx_latest_praybeyt_benjamin_1])\n",
    "\n",
    "print(\"\\nPraybeyt Benjamin 1:\")\n",
    "print(f\"ltbx_earliest_praybeyt_benjamin_1: {ltbx_earliest_praybeyt_benjamin_1.shape}\")\n",
    "print(f\"ltbx_latest_praybeyt_benjamin_1: {ltbx_latest_praybeyt_benjamin_1.shape}\")\n",
    "print(f\"rt_praybeyt_benjamin_1: {rt_praybeyt_benjamin_1.shape}\")\n",
    "print(f\"ltbx_praybeyt_benjamin_1: {ltbx_praybeyt_benjamin_1.shape}\")\n",
    "\n",
    "# Mallari - Complete\n",
    "imdb_mallari = pd.read_csv(\"./Unclean Data/Mallari/imdb_mallari.csv\")\n",
    "ltbx_earliest_mallari = pd.read_csv(\"./Unclean Data/Mallari/ltbx_earliest_mallari.csv\")\n",
    "ltbx_latest_mallari = pd.read_csv(\"./Unclean Data/Mallari/ltbx_latest_mallari.csv\")\n",
    "rt_mallari = pd.read_csv(\"./Unclean Data/Mallari/rt_mallari.csv\")\n",
    "\n",
    "ltbx_mallari = pd.concat([ltbx_earliest_mallari, ltbx_latest_mallari])\n",
    "\n",
    "print(\"\\nMallari:\")\n",
    "print(f\"imdb_mallari: {imdb_mallari.shape}\")\n",
    "print(f\"ltbx_earliest_mallari: {ltbx_earliest_mallari.shape}\")\n",
    "print(f\"ltbx_latest_mallari: {ltbx_latest_mallari.shape}\")\n",
    "print(f\"rt_mallari: {rt_mallari.shape}\")\n",
    "print(f\"ltbx_mallari: {ltbx_mallari.shape}\")\n",
    "\n",
    "# Maid in Malacanang - No RT\n",
    "imdb_maid_in_malacanang = pd.read_csv(\"./Unclean Data/Maid in Malacañang/imdb_maid_in_malacanang.csv\")\n",
    "ltbx_earliest_maid_in_malacanang = pd.read_csv(\"./Unclean Data/Maid in Malacañang/ltbx_earliest_maid_in_malacanang.csv\")\n",
    "ltbx_latest_maid_in_malacanang = pd.read_csv(\"./Unclean Data/Maid in Malacañang/ltbx_latest_maid_in_malacanang.csv\")\n",
    "\n",
    "ltbx_maid_in_malacanang = pd.concat([ltbx_earliest_maid_in_malacanang, ltbx_latest_maid_in_malacanang])\n",
    "\n",
    "print(\"\\nMaid in Malacañang:\")\n",
    "print(f\"imdb_maid_in_malacanang: {imdb_maid_in_malacanang.shape}\")\n",
    "print(f\"ltbx_earliest_maid_in_malacanang: {ltbx_earliest_maid_in_malacanang.shape}\")\n",
    "print(f\"ltbx_latest_maid_in_malacanang: {ltbx_latest_maid_in_malacanang.shape}\")\n",
    "print(f\"ltbx_maid_in_malacanang: {ltbx_maid_in_malacanang.shape}\")\n",
    "\n",
    "# Hows of Us - Complete\n",
    "imdb_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/imdb_hows_of_us.csv\")\n",
    "ltbx_earliest_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/ltbx_earliest_hows_of_us.csv\")\n",
    "ltbx_latest_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/ltbx_latest_hows_of_us.csv\")\n",
    "rt_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/rt_hows_of_us.csv\")\n",
    "\n",
    "ltbx_hows_of_us = pd.concat([ltbx_earliest_hows_of_us, ltbx_latest_hows_of_us])\n",
    "\n",
    "print(\"\\nHows of Us:\")\n",
    "print(f\"imdb_hows_of_us: {imdb_hows_of_us.shape}\")\n",
    "print(f\"ltbx_earliest_hows_of_us: {ltbx_earliest_hows_of_us.shape}\")\n",
    "print(f\"ltbx_latest_hows_of_us: {ltbx_latest_hows_of_us.shape}\")\n",
    "print(f\"rt_hows_of_us: {rt_hows_of_us.shape}\")\n",
    "print(f\"ltbx_hows_of_us: {ltbx_hows_of_us.shape}\")\n",
    "\n",
    "# Felix Manalo - Complete\n",
    "ltbx_earliest_felix_manalo = pd.read_csv(\"./Unclean Data/Felix Manalo/ltbx_earliest_felix_manalo.csv\")\n",
    "ltbx_latest_felix_manalo = pd.read_csv(\"./Unclean Data/Felix Manalo/ltbx_latest_felix_manalo.csv\")\n",
    "rt_felix_manalo = pd.read_csv(\"./Unclean Data/Felix Manalo/rt_felix_manalo.csv\")\n",
    "\n",
    "ltbx_felix_manalo = pd.concat([ltbx_earliest_felix_manalo, ltbx_latest_felix_manalo])\n",
    "\n",
    "print(\"\\nFelix Manalo:\")\n",
    "print(f\"ltbx_earliest_felix_manalo: {ltbx_earliest_felix_manalo.shape}\")\n",
    "print(f\"ltbx_latest_felix_manalo: {ltbx_latest_felix_manalo.shape}\")\n",
    "print(f\"rt_felix_manalo: {rt_felix_manalo.shape}\")\n",
    "print(f\"ltbx_felix_manalo: {ltbx_felix_manalo.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec2c23",
   "metadata": {},
   "source": [
    "### 2.0. Removal of Duplicate Data\n",
    "A row is considered a duplicate if:\n",
    "- `ltbx` - if username, date, review_type, and review_text are the same\n",
    "- `rt` - if Name, Date, and Review Content are the same\n",
    "- `imdb` - if Username, Date, and Review Text are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6081b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The Kingmaker ===\n",
      "imdb: 23 -> 23 (removed 0)\n",
      "ltbx: 1452 -> 1452 (removed 0)\n",
      "rt: 73 -> 73 (removed 0)\n",
      "\n",
      "=== Praybeyt Benjamin 2 ===\n",
      "imdb: 6 -> 6 (removed 0)\n",
      "ltbx: 288 -> 274 (removed 14)\n",
      "\n",
      "=== Hello Love Goodbye ===\n",
      "imdb: 44 -> 44 (removed 0)\n",
      "ltbx: 3096 -> 3095 (removed 1)\n",
      "rt: 10 -> 10 (removed 0)\n",
      "\n",
      "=== Hayop Ka ===\n",
      "imdb: 24 -> 24 (removed 0)\n",
      "ltbx: 912 -> 898 (removed 14)\n",
      "rt: 8 -> 8 (removed 0)\n",
      "\n",
      "=== Sunshine ===\n",
      "imdb: 15 -> 15 (removed 0)\n",
      "ltbx: 6144 -> 6140 (removed 4)\n",
      "\n",
      "=== Quezon ===\n",
      "imdb: 8 -> 8 (removed 0)\n",
      "ltbx: 2808 -> 2787 (removed 21)\n",
      "rt: 3 -> 3 (removed 0)\n",
      "\n",
      "=== Praybeyt Benjamin 1 ===\n",
      "imdb: 3 -> 3 (removed 0)\n",
      "ltbx: 4800 -> 605 (removed 4195)\n",
      "rt: 19 -> 19 (removed 0)\n",
      "\n",
      "=== Mallari ===\n",
      "imdb: 18 -> 18 (removed 0)\n",
      "ltbx: 6000 -> 5975 (removed 25)\n",
      "rt: 9 -> 9 (removed 0)\n",
      "\n",
      "=== Maid in Malacañang ===\n",
      "imdb: 1339 -> 1339 (removed 0)\n",
      "ltbx: 672 -> 668 (removed 4)\n",
      "\n",
      "=== Hows of Us ===\n",
      "imdb: 58 -> 58 (removed 0)\n",
      "ltbx: 4440 -> 4426 (removed 14)\n",
      "rt: 8 -> 8 (removed 0)\n",
      "\n",
      "=== Felix Manalo ===\n",
      "ltbx: 64 -> 40 (removed 24)\n",
      "rt: 3 -> 3 (removed 0)\n",
      "\n",
      "================================================================================\n",
      "All 11 movies cleaned successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean The Kingmaker\n",
    "print(\"=== The Kingmaker ===\")\n",
    "imdb_before = imdb_the_kingmaker.shape[0]\n",
    "ltbx_before = ltbx_the_kingmaker.shape[0]\n",
    "rt_before = rt_the_kingmaker.shape[0]\n",
    "\n",
    "imdb_the_kingmaker = imdb_the_kingmaker.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_the_kingmaker = ltbx_the_kingmaker.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_the_kingmaker = rt_the_kingmaker.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_the_kingmaker.shape[0]} (removed {imdb_before - imdb_the_kingmaker.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_the_kingmaker.shape[0]} (removed {ltbx_before - ltbx_the_kingmaker.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_the_kingmaker.shape[0]} (removed {rt_before - rt_the_kingmaker.shape[0]})\")\n",
    "\n",
    "# Clean Praybeyt Benjamin 2\n",
    "print(\"\\n=== Praybeyt Benjamin 2 ===\")\n",
    "imdb_before = imdb_praybeyt_benjamin_2.shape[0]\n",
    "ltbx_before = ltbx_praybeyt_benjamin_2.shape[0]\n",
    "\n",
    "imdb_praybeyt_benjamin_2 = imdb_praybeyt_benjamin_2.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_praybeyt_benjamin_2 = ltbx_praybeyt_benjamin_2.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_praybeyt_benjamin_2.shape[0]} (removed {imdb_before - imdb_praybeyt_benjamin_2.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_praybeyt_benjamin_2.shape[0]} (removed {ltbx_before - ltbx_praybeyt_benjamin_2.shape[0]})\")\n",
    "\n",
    "# Clean Hello Love Goodbye\n",
    "print(\"\\n=== Hello Love Goodbye ===\")\n",
    "imdb_before = imdb_hello_love_goodbye.shape[0]\n",
    "ltbx_before = ltbx_hello_love_goodbye.shape[0]\n",
    "rt_before = rt_hello_love_goodbye.shape[0]\n",
    "\n",
    "imdb_hello_love_goodbye = imdb_hello_love_goodbye.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_hello_love_goodbye = ltbx_hello_love_goodbye.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_hello_love_goodbye = rt_hello_love_goodbye.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_hello_love_goodbye.shape[0]} (removed {imdb_before - imdb_hello_love_goodbye.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_hello_love_goodbye.shape[0]} (removed {ltbx_before - ltbx_hello_love_goodbye.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_hello_love_goodbye.shape[0]} (removed {rt_before - rt_hello_love_goodbye.shape[0]})\")\n",
    "\n",
    "# Clean Hayop Ka\n",
    "print(\"\\n=== Hayop Ka ===\")\n",
    "imdb_before = imdb_hayop_ka.shape[0]\n",
    "ltbx_before = ltbx_hayop_ka.shape[0]\n",
    "rt_before = rt_hayop_ka.shape[0]\n",
    "\n",
    "imdb_hayop_ka = imdb_hayop_ka.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_hayop_ka = ltbx_hayop_ka.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_hayop_ka = rt_hayop_ka.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_hayop_ka.shape[0]} (removed {imdb_before - imdb_hayop_ka.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_hayop_ka.shape[0]} (removed {ltbx_before - ltbx_hayop_ka.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_hayop_ka.shape[0]} (removed {rt_before - rt_hayop_ka.shape[0]})\")\n",
    "\n",
    "# Clean Sunshine\n",
    "print(\"\\n=== Sunshine ===\")\n",
    "imdb_before = imdb_sunshine.shape[0]\n",
    "ltbx_before = ltbx_sunshine.shape[0]\n",
    "\n",
    "imdb_sunshine = imdb_sunshine.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_sunshine = ltbx_sunshine.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_sunshine.shape[0]} (removed {imdb_before - imdb_sunshine.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_sunshine.shape[0]} (removed {ltbx_before - ltbx_sunshine.shape[0]})\")\n",
    "\n",
    "# Clean Quezon\n",
    "print(\"\\n=== Quezon ===\")\n",
    "imdb_before = imdb_quezon.shape[0]\n",
    "ltbx_before = ltbx_quezon.shape[0]\n",
    "rt_before = rt_quezon.shape[0]\n",
    "\n",
    "imdb_quezon = imdb_quezon.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_quezon = ltbx_quezon.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_quezon = rt_quezon.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_quezon.shape[0]} (removed {imdb_before - imdb_quezon.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_quezon.shape[0]} (removed {ltbx_before - ltbx_quezon.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_quezon.shape[0]} (removed {rt_before - rt_quezon.shape[0]})\")\n",
    "\n",
    "# Clean Praybeyt Benjamin 1\n",
    "print(\"\\n=== Praybeyt Benjamin 1 ===\")\n",
    "imdb_before = imdb_praybeyt_benjamin_1.shape[0]\n",
    "ltbx_before = ltbx_praybeyt_benjamin_1.shape[0]\n",
    "rt_before = rt_praybeyt_benjamin_1.shape[0]\n",
    "\n",
    "imdb_praybeyt_benjamin_1 = imdb_praybeyt_benjamin_1.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_praybeyt_benjamin_1 = ltbx_praybeyt_benjamin_1.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_praybeyt_benjamin_1 = rt_praybeyt_benjamin_1.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_praybeyt_benjamin_1.shape[0]} (removed {imdb_before - imdb_praybeyt_benjamin_1.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_praybeyt_benjamin_1.shape[0]} (removed {ltbx_before - ltbx_praybeyt_benjamin_1.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_praybeyt_benjamin_1.shape[0]} (removed {rt_before - rt_praybeyt_benjamin_1.shape[0]})\")\n",
    "\n",
    "# Clean Mallari\n",
    "print(\"\\n=== Mallari ===\")\n",
    "imdb_before = imdb_mallari.shape[0]\n",
    "ltbx_before = ltbx_mallari.shape[0]\n",
    "rt_before = rt_mallari.shape[0]\n",
    "\n",
    "imdb_mallari = imdb_mallari.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_mallari = ltbx_mallari.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_mallari = rt_mallari.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_mallari.shape[0]} (removed {imdb_before - imdb_mallari.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_mallari.shape[0]} (removed {ltbx_before - ltbx_mallari.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_mallari.shape[0]} (removed {rt_before - rt_mallari.shape[0]})\")\n",
    "\n",
    "# Clean Maid in Malacañang\n",
    "print(\"\\n=== Maid in Malacañang ===\")\n",
    "imdb_before = imdb_maid_in_malacanang.shape[0]\n",
    "ltbx_before = ltbx_maid_in_malacanang.shape[0]\n",
    "\n",
    "imdb_maid_in_malacanang = imdb_maid_in_malacanang.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_maid_in_malacanang = ltbx_maid_in_malacanang.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_maid_in_malacanang.shape[0]} (removed {imdb_before - imdb_maid_in_malacanang.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_maid_in_malacanang.shape[0]} (removed {ltbx_before - ltbx_maid_in_malacanang.shape[0]})\")\n",
    "\n",
    "# Clean Hows of Us\n",
    "print(\"\\n=== Hows of Us ===\")\n",
    "imdb_before = imdb_hows_of_us.shape[0]\n",
    "ltbx_before = ltbx_hows_of_us.shape[0]\n",
    "rt_before = rt_hows_of_us.shape[0]\n",
    "\n",
    "imdb_hows_of_us = imdb_hows_of_us.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_hows_of_us = ltbx_hows_of_us.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_hows_of_us = rt_hows_of_us.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_hows_of_us.shape[0]} (removed {imdb_before - imdb_hows_of_us.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_hows_of_us.shape[0]} (removed {ltbx_before - ltbx_hows_of_us.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_hows_of_us.shape[0]} (removed {rt_before - rt_hows_of_us.shape[0]})\")\n",
    "\n",
    "# Clean Felix Manalo\n",
    "print(\"\\n=== Felix Manalo ===\")\n",
    "ltbx_before = ltbx_felix_manalo.shape[0]\n",
    "rt_before = rt_felix_manalo.shape[0]\n",
    "\n",
    "ltbx_felix_manalo = ltbx_felix_manalo.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_felix_manalo = rt_felix_manalo.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_felix_manalo.shape[0]} (removed {ltbx_before - ltbx_felix_manalo.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_felix_manalo.shape[0]} (removed {rt_before - rt_felix_manalo.shape[0]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All 11 movies cleaned successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cbc509",
   "metadata": {},
   "source": [
    "### 3.0 Removal of Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a7472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis from text\n",
    "def remove_emojis(text):\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    # Emoji pattern\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # Chinese characters\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b694b20",
   "metadata": {},
   "source": [
    "### 4.0 Combining of reviews\n",
    "- per movie \n",
    "- an all reviews file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b93ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_reviews_df(imdb_df, ltbx_df, rt_df):\n",
    "\n",
    "    # Create empty list to store all reviews\n",
    "    all_reviews = []\n",
    "    \n",
    "    # Process IMDb reviews\n",
    "    if imdb_df is not None and not imdb_df.empty:\n",
    "        for _, row in imdb_df.iterrows():\n",
    "            # Create review text: title + review text\n",
    "            title = row['Title'] if pd.notna(row['Title']) else ''\n",
    "            review_text_raw = row.get('Review Text', '')\n",
    "            \n",
    "            # Combine title and review text\n",
    "            if title and review_text_raw:\n",
    "                review_text = f\"{title}: {review_text_raw}\"\n",
    "            elif review_text_raw:\n",
    "                review_text = review_text_raw\n",
    "            else:\n",
    "                review_text = ''\n",
    "            \n",
    "            # Remove emojis from review text\n",
    "            review_text = remove_emojis(review_text)\n",
    "            \n",
    "            # Convert rating to percentage (IMDb uses 1-10 scale)\n",
    "            try:\n",
    "                # Handle different formats: \"7/10\", \"7.5/10\", \"7\", 7.5\n",
    "                if isinstance(row['Rating'], str):\n",
    "                    if '/' in row['Rating']:\n",
    "                        num, denom = row['Rating'].split('/')\n",
    "                        rating = (float(num) / float(denom)) * 100\n",
    "                    else:\n",
    "                        rating = float(row['Rating']) * 10  # Convert 0-10 to 0-100\n",
    "                else:\n",
    "                    rating = float(row['Rating']) * 10  # Convert 0-10 to 0-100\n",
    "            except:\n",
    "                rating = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                'username': row['Username'],\n",
    "                'date': row['Date'],\n",
    "                'review_text': review_text,\n",
    "                'rating': rating,\n",
    "                'source': 'imdb'\n",
    "            })\n",
    "    \n",
    "    # Process Letterboxd reviews\n",
    "    if ltbx_df is not None and not ltbx_df.empty:\n",
    "        for _, row in ltbx_df.iterrows():\n",
    "            # Get review text and remove emojis\n",
    "            review_text = row.get('review_text', '')\n",
    "            review_text = remove_emojis(review_text)\n",
    "            \n",
    "            # Convert rating to percentage (Letterboxd uses 0.5-5 scale)\n",
    "            try:\n",
    "                if pd.notna(row['rating']):\n",
    "                    # Handle different formats: 4.0, 4.5, \"4\", \"4.5\"\n",
    "                    rating = (float(row['rating']) / 5) * 100\n",
    "                else:\n",
    "                    rating = None\n",
    "            except:\n",
    "                rating = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                'username': row['username'],\n",
    "                'date': row['date'],\n",
    "                'review_text': review_text,\n",
    "                'rating': rating,\n",
    "                'source': 'ltbx'\n",
    "            })\n",
    "    \n",
    "    # Process Rotten Tomatoes reviews\n",
    "    if rt_df is not None and not rt_df.empty:\n",
    "        for _, row in rt_df.iterrows():\n",
    "            # Get review text and remove emojis\n",
    "            review_text = row.get('Review Content', '')\n",
    "            review_text = remove_emojis(review_text)\n",
    "            \n",
    "            # Convert rating to percentage\n",
    "            try:\n",
    "                if pd.notna(row['Rating']):\n",
    "                    # Handle different formats: \"4/5\", \"3.5/5\", \"Fresh\", \"Rotten\"\n",
    "                    if isinstance(row['Rating'], str):\n",
    "                        if '/' in row['Rating']:\n",
    "                            num, denom = row['Rating'].split('/')\n",
    "                            rating = (float(num) / float(denom)) * 100\n",
    "                        elif row['Rating'].lower() == 'fresh':\n",
    "                            rating = 100  # Fresh = positive = 100%\n",
    "                        elif row['Rating'].lower() == 'rotten':\n",
    "                            rating = 0    # Rotten = negative = 0%\n",
    "                        else:\n",
    "                            rating = None\n",
    "                    else:\n",
    "                        rating = float(row['Rating'])  # Assume already in percentage\n",
    "                else:\n",
    "                    rating = None\n",
    "            except:\n",
    "                rating = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                'username': row['Name'],\n",
    "                'date': row['Date'],\n",
    "                'review_text': review_text,\n",
    "                'rating': rating,\n",
    "                'source': 'rt'\n",
    "            })\n",
    "    \n",
    "    # Create dataframe\n",
    "    reviews_df = pd.DataFrame(all_reviews)\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    if 'date' in reviews_df.columns and not reviews_df.empty:\n",
    "        reviews_df['date'] = pd.to_datetime(reviews_df['date'], errors='coerce')\n",
    "    \n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c36bd",
   "metadata": {},
   "source": [
    "### 5.0. Export to CSV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92859a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_reviews_to_csv(reviews_df_dict, cleaned_data_dir=\"./Cleaned Data\"):\n",
    "    \"\"\"\n",
    "    Export all review dataframes to CSV files\n",
    "    \n",
    "    Args:\n",
    "        reviews_df_dict: Dictionary of review dataframes with movie names as keys\n",
    "        cleaned_data_dir: Directory to save CSV files\n",
    "    \"\"\"\n",
    "    # Create Cleaned Data directory if it doesn't exist\n",
    "    if not os.path.exists(cleaned_data_dir):\n",
    "        os.makedirs(cleaned_data_dir)\n",
    "    \n",
    "    # Export individual movie review files\n",
    "    print(\"Exporting individual movie review files...\")\n",
    "    for movie_name, reviews_df in reviews_df_dict.items():\n",
    "        # Create filename from movie name\n",
    "        filename = movie_name.lower().replace(' ', '_').replace('ñ', 'n').replace('!', '')\n",
    "        filename = f\"{filename}_reviews.csv\"\n",
    "        filepath = os.path.join(cleaned_data_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            reviews_df.to_csv(filepath, index=False)\n",
    "            print(f\"✓ {movie_name}: {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {movie_name}: Error - {e}\")\n",
    "    \n",
    "    # Create combined reviews.csv\n",
    "    print(\"\\nCreating combined reviews.csv file...\")\n",
    "    \n",
    "    # Add movie_name column to each dataframe and combine\n",
    "    all_reviews_dfs = []\n",
    "    for movie_name, reviews_df in reviews_df_dict.items():\n",
    "        if not reviews_df.empty:\n",
    "            reviews_df_with_movie = reviews_df.copy()\n",
    "            reviews_df_with_movie['movie_name'] = movie_name\n",
    "            all_reviews_dfs.append(reviews_df_with_movie)\n",
    "    \n",
    "    if all_reviews_dfs:\n",
    "        combined_reviews_df = pd.concat(all_reviews_dfs, ignore_index=True)\n",
    "        \n",
    "        # Reorder columns: movie_name, username, date, review_text, rating, source\n",
    "        if not combined_reviews_df.empty:\n",
    "            combined_reviews_df = combined_reviews_df[['movie_name', 'username', 'date', 'review_text', 'rating', 'source']]\n",
    "            \n",
    "            # Save combined reviews.csv\n",
    "            combined_filepath = os.path.join(cleaned_data_dir, \"reviews.csv\")\n",
    "            combined_reviews_df.to_csv(combined_filepath, index=False)\n",
    "            \n",
    "            print(f\"\\n✓ Combined file: {combined_filepath}\")\n",
    "            print(f\"  Total reviews: {combined_reviews_df.shape[0]}\")\n",
    "            print(f\"  Movies included: {combined_reviews_df['movie_name'].nunique()}\")\n",
    "            print(f\"  Sources: {combined_reviews_df['source'].value_counts().to_dict()}\")\n",
    "        else:\n",
    "            print(\"✗ No reviews to combine\")\n",
    "    else:\n",
    "        print(\"✗ No review dataframes to combine\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Export completed!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b8027d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Review DataFrames Summary:\n",
      "================================================================================\n",
      "The Kingmaker: 1548 reviews\n",
      "  Sources: {'ltbx': 1452, 'rt': 73, 'imdb': 23}\n",
      "\n",
      "Praybeyt Benjamin 2: 280 reviews\n",
      "  Sources: {'ltbx': 274, 'imdb': 6}\n",
      "\n",
      "Hello Love Goodbye: 3149 reviews\n",
      "  Sources: {'ltbx': 3095, 'imdb': 44, 'rt': 10}\n",
      "\n",
      "Hayop Ka: 930 reviews\n",
      "  Sources: {'ltbx': 898, 'imdb': 24, 'rt': 8}\n",
      "\n",
      "Sunshine: 6155 reviews\n",
      "  Sources: {'ltbx': 6140, 'imdb': 15}\n",
      "\n",
      "Quezon: 2798 reviews\n",
      "  Sources: {'ltbx': 2787, 'imdb': 8, 'rt': 3}\n",
      "\n",
      "Praybeyt Benjamin 1: 627 reviews\n",
      "  Sources: {'ltbx': 605, 'rt': 19, 'imdb': 3}\n",
      "\n",
      "Mallari: 6002 reviews\n",
      "  Sources: {'ltbx': 5975, 'imdb': 18, 'rt': 9}\n",
      "\n",
      "Maid in Malacañang: 2007 reviews\n",
      "  Sources: {'imdb': 1339, 'ltbx': 668}\n",
      "\n",
      "Hows of Us: 4492 reviews\n",
      "  Sources: {'ltbx': 4426, 'imdb': 58, 'rt': 8}\n",
      "\n",
      "Felix Manalo: 43 reviews\n",
      "  Sources: {'ltbx': 40, 'rt': 3}\n"
     ]
    }
   ],
   "source": [
    "# Create combined review dataframes for all movies\n",
    "the_kingmaker_reviews_df = create_combined_reviews_df(imdb_the_kingmaker, ltbx_the_kingmaker, rt_the_kingmaker)\n",
    "praybeyt_benjamin_2_reviews_df = create_combined_reviews_df(imdb_praybeyt_benjamin_2, ltbx_praybeyt_benjamin_2, None)\n",
    "hello_love_goodbye_reviews_df = create_combined_reviews_df(imdb_hello_love_goodbye, ltbx_hello_love_goodbye, rt_hello_love_goodbye)\n",
    "hayop_ka_reviews_df = create_combined_reviews_df(imdb_hayop_ka, ltbx_hayop_ka, rt_hayop_ka)\n",
    "sunshine_reviews_df = create_combined_reviews_df(imdb_sunshine, ltbx_sunshine, None)\n",
    "quezon_reviews_df = create_combined_reviews_df(imdb_quezon, ltbx_quezon, rt_quezon)\n",
    "praybeyt_benjamin_1_reviews_df = create_combined_reviews_df(imdb_praybeyt_benjamin_1, ltbx_praybeyt_benjamin_1, rt_praybeyt_benjamin_1)\n",
    "mallari_reviews_df = create_combined_reviews_df(imdb_mallari, ltbx_mallari, rt_mallari)\n",
    "maid_in_malacanang_reviews_df = create_combined_reviews_df(imdb_maid_in_malacanang, ltbx_maid_in_malacanang, None)\n",
    "hows_of_us_reviews_df = create_combined_reviews_df(imdb_hows_of_us, ltbx_hows_of_us, rt_hows_of_us)\n",
    "felix_manalo_reviews_df = create_combined_reviews_df(None, ltbx_felix_manalo, rt_felix_manalo)\n",
    "\n",
    "# Print summary for each movie\n",
    "print(\"Combined Review DataFrames Summary:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"The Kingmaker: {the_kingmaker_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {the_kingmaker_reviews_df['source'].value_counts().to_dict() if not the_kingmaker_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nPraybeyt Benjamin 2: {praybeyt_benjamin_2_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {praybeyt_benjamin_2_reviews_df['source'].value_counts().to_dict() if not praybeyt_benjamin_2_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nHello Love Goodbye: {hello_love_goodbye_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {hello_love_goodbye_reviews_df['source'].value_counts().to_dict() if not hello_love_goodbye_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nHayop Ka: {hayop_ka_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {hayop_ka_reviews_df['source'].value_counts().to_dict() if not hayop_ka_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nSunshine: {sunshine_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {sunshine_reviews_df['source'].value_counts().to_dict() if not sunshine_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nQuezon: {quezon_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {quezon_reviews_df['source'].value_counts().to_dict() if not quezon_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nPraybeyt Benjamin 1: {praybeyt_benjamin_1_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {praybeyt_benjamin_1_reviews_df['source'].value_counts().to_dict() if not praybeyt_benjamin_1_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nMallari: {mallari_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {mallari_reviews_df['source'].value_counts().to_dict() if not mallari_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nMaid in Malacañang: {maid_in_malacanang_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {maid_in_malacanang_reviews_df['source'].value_counts().to_dict() if not maid_in_malacanang_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nHows of Us: {hows_of_us_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {hows_of_us_reviews_df['source'].value_counts().to_dict() if not hows_of_us_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nFelix Manalo: {felix_manalo_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {felix_manalo_reviews_df['source'].value_counts().to_dict() if not felix_manalo_reviews_df.empty else {}}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9572fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating combined reviews.csv file...\n",
      "\n",
      "================================================================================\n",
      "All review dataframes exported:\n",
      "================================================================================\n",
      "Individual movie files:\n",
      "1. ./Cleaned Data/the_kingmaker_reviews.csv\n",
      "2. ./Cleaned Data/praybeyt_benjamin_2_reviews.csv\n",
      "3. ./Cleaned Data/hello_love_goodbye_reviews.csv\n",
      "4. ./Cleaned Data/hayop_ka_reviews.csv\n",
      "5. ./Cleaned Data/sunshine_reviews.csv\n",
      "6. ./Cleaned Data/quezon_reviews.csv\n",
      "7. ./Cleaned Data/praybeyt_benjamin_1_reviews.csv\n",
      "8. ./Cleaned Data/mallari_reviews.csv\n",
      "9. ./Cleaned Data/maid_in_malacanang_reviews.csv\n",
      "10. ./Cleaned Data/hows_of_us_reviews.csv\n",
      "11. ./Cleaned Data/felix_manalo_reviews.csv\n",
      "\n",
      "Combined file:\n",
      "✓ ./Cleaned Data/reviews.csv (All 11 movies combined)\n",
      "\n",
      "Total reviews in combined file: 28031\n",
      "Movies in combined file: 11\n",
      "Sources in combined file: {'ltbx': 26360, 'imdb': 1538, 'rt': 133}\n",
      "\n",
      "================================================================================\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create Cleaned Data directory if it doesn't exist\n",
    "cleaned_data_dir = \"./Cleaned Data\"\n",
    "if not os.path.exists(cleaned_data_dir):\n",
    "    os.makedirs(cleaned_data_dir)\n",
    "\n",
    "# First export individual movie review files\n",
    "the_kingmaker_reviews_df.to_csv(f\"{cleaned_data_dir}/the_kingmaker_reviews.csv\", index=False)\n",
    "praybeyt_benjamin_2_reviews_df.to_csv(f\"{cleaned_data_dir}/praybeyt_benjamin_2_reviews.csv\", index=False)\n",
    "hello_love_goodbye_reviews_df.to_csv(f\"{cleaned_data_dir}/hello_love_goodbye_reviews.csv\", index=False)\n",
    "hayop_ka_reviews_df.to_csv(f\"{cleaned_data_dir}/hayop_ka_reviews.csv\", index=False)\n",
    "sunshine_reviews_df.to_csv(f\"{cleaned_data_dir}/sunshine_reviews.csv\", index=False)\n",
    "quezon_reviews_df.to_csv(f\"{cleaned_data_dir}/quezon_reviews.csv\", index=False)\n",
    "praybeyt_benjamin_1_reviews_df.to_csv(f\"{cleaned_data_dir}/praybeyt_benjamin_1_reviews.csv\", index=False)\n",
    "mallari_reviews_df.to_csv(f\"{cleaned_data_dir}/mallari_reviews.csv\", index=False)\n",
    "maid_in_malacanang_reviews_df.to_csv(f\"{cleaned_data_dir}/maid_in_malacanang_reviews.csv\", index=False)\n",
    "hows_of_us_reviews_df.to_csv(f\"{cleaned_data_dir}/hows_of_us_reviews.csv\", index=False)\n",
    "felix_manalo_reviews_df.to_csv(f\"{cleaned_data_dir}/felix_manalo_reviews.csv\", index=False)\n",
    "\n",
    "# Create a combined reviews.csv with all movies\n",
    "print(\"Creating combined reviews.csv file...\")\n",
    "\n",
    "# Combine all reviews dataframes and add movie_name column\n",
    "all_reviews_dfs = []\n",
    "\n",
    "# Add movie name to each dataframe and append to list\n",
    "the_kingmaker_reviews_df['movie_name'] = 'The Kingmaker'\n",
    "praybeyt_benjamin_2_reviews_df['movie_name'] = 'Praybeyt Benjamin 2'\n",
    "hello_love_goodbye_reviews_df['movie_name'] = 'Hello Love Goodbye'\n",
    "hayop_ka_reviews_df['movie_name'] = 'Hayop Ka'\n",
    "sunshine_reviews_df['movie_name'] = 'Sunshine'\n",
    "quezon_reviews_df['movie_name'] = 'Quezon'\n",
    "praybeyt_benjamin_1_reviews_df['movie_name'] = 'Praybeyt Benjamin 1'\n",
    "mallari_reviews_df['movie_name'] = 'Mallari'\n",
    "maid_in_malacanang_reviews_df['movie_name'] = 'Maid in Malacañang'\n",
    "hows_of_us_reviews_df['movie_name'] = 'Hows of Us'\n",
    "felix_manalo_reviews_df['movie_name'] = 'Felix Manalo'\n",
    "\n",
    "# Append all dataframes\n",
    "all_reviews_dfs.extend([\n",
    "    the_kingmaker_reviews_df,\n",
    "    praybeyt_benjamin_2_reviews_df,\n",
    "    hello_love_goodbye_reviews_df,\n",
    "    hayop_ka_reviews_df,\n",
    "    sunshine_reviews_df,\n",
    "    quezon_reviews_df,\n",
    "    praybeyt_benjamin_1_reviews_df,\n",
    "    mallari_reviews_df,\n",
    "    maid_in_malacanang_reviews_df,\n",
    "    hows_of_us_reviews_df,\n",
    "    felix_manalo_reviews_df\n",
    "])\n",
    "\n",
    "# Concatenate all dataframes\n",
    "combined_reviews_df = pd.concat(all_reviews_dfs, ignore_index=True)\n",
    "\n",
    "# Reorder columns: movie_name, username, date, review_text, rating, source\n",
    "combined_reviews_df = combined_reviews_df[['movie_name', 'username', 'date', 'review_text', 'rating', 'source']]\n",
    "\n",
    "# Save combined reviews.csv\n",
    "combined_reviews_df.to_csv(f\"{cleaned_data_dir}/reviews.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All review dataframes exported:\")\n",
    "print(\"=\"*80)\n",
    "print(\"Individual movie files:\")\n",
    "print(f\"1. {cleaned_data_dir}/the_kingmaker_reviews.csv\")\n",
    "print(f\"2. {cleaned_data_dir}/praybeyt_benjamin_2_reviews.csv\")\n",
    "print(f\"3. {cleaned_data_dir}/hello_love_goodbye_reviews.csv\")\n",
    "print(f\"4. {cleaned_data_dir}/hayop_ka_reviews.csv\")\n",
    "print(f\"5. {cleaned_data_dir}/sunshine_reviews.csv\")\n",
    "print(f\"6. {cleaned_data_dir}/quezon_reviews.csv\")\n",
    "print(f\"7. {cleaned_data_dir}/praybeyt_benjamin_1_reviews.csv\")\n",
    "print(f\"8. {cleaned_data_dir}/mallari_reviews.csv\")\n",
    "print(f\"9. {cleaned_data_dir}/maid_in_malacanang_reviews.csv\")\n",
    "print(f\"10. {cleaned_data_dir}/hows_of_us_reviews.csv\")\n",
    "print(f\"11. {cleaned_data_dir}/felix_manalo_reviews.csv\")\n",
    "\n",
    "print(\"\\nCombined file:\")\n",
    "print(f\"✓ {cleaned_data_dir}/reviews.csv (All 11 movies combined)\")\n",
    "print(f\"\\nTotal reviews in combined file: {combined_reviews_df.shape[0]}\")\n",
    "print(f\"Movies in combined file: {combined_reviews_df['movie_name'].nunique()}\")\n",
    "print(f\"Sources in combined file: {combined_reviews_df['source'].value_counts().to_dict()}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Files saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d06357",
   "metadata": {},
   "source": [
    "### 6.0 Sentence Splitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22955fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_reviews_into_sentences(reviews_df, movie_name):\n",
    "    \n",
    "    if reviews_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    sentences_data = []\n",
    "    \n",
    "    for idx, row in reviews_df.iterrows():\n",
    "        review_text = str(row.get('review_text', ''))\n",
    "        username = row.get('username', '')\n",
    "        date = row.get('date', '')\n",
    "        rating = row.get('rating', None)\n",
    "        source = row.get('source', '')\n",
    "        \n",
    "        # Skip empty review text\n",
    "        if not review_text or review_text == 'nan':\n",
    "            continue\n",
    "        \n",
    "        # Split into sentences\n",
    "        try:\n",
    "            review_sentences = sent_tokenize(review_text)\n",
    "        except:\n",
    "            # Fallback simple sentence splitting\n",
    "            review_sentences = re.split(r'[.!?]+', review_text)\n",
    "            review_sentences = [s.strip() for s in review_sentences if s.strip()]\n",
    "        \n",
    "        # Add each sentence as a separate row\n",
    "        for sentence_num, sentence in enumerate(review_sentences, 1):\n",
    "            # Skip very short sentences (likely not meaningful)\n",
    "            if len(sentence.split()) < 2:\n",
    "                continue\n",
    "                \n",
    "            sentences_data.append({\n",
    "                'movie_name': movie_name,\n",
    "                'review_id': idx,\n",
    "                'sentence_num': sentence_num,\n",
    "                'username': username,\n",
    "                'date': date,\n",
    "                'rating': rating,\n",
    "                'source': source,\n",
    "                'sentence': sentence.strip()\n",
    "            })\n",
    "    \n",
    "    sentences_df = pd.DataFrame(sentences_data)\n",
    "    \n",
    "    return sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1856b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting The Kingmaker reviews into sentences...\n",
      "Original reviews: 1548\n",
      "Sentences extracted: 5006\n",
      "Average sentences per review: 3.23\n",
      "\n",
      "Sample sentences:\n",
      "   sentence_num                                           sentence source\n",
      "0             1  This is the truth: This documentary perfectly ...   imdb\n",
      "1             2  It showed what it is like to live with extensi...   imdb\n",
      "2             3  This is a must watch film for those who wish t...   imdb\n",
      "3             4  This documentary shows the truth, through dire...   imdb\n",
      "4             5  As John Henry Newman would say: \"We can believ...   imdb\n",
      "5             1  Prescient and illuminating: As I write Ferdina...   imdb\n",
      "6             2  It's a shocking tale of shameless corruption n...   imdb\n",
      "7             3  Half of todays Filipino voters were not alive ...   imdb\n",
      "8             4  Sadly all of that now seems to have been forgo...   imdb\n",
      "9             5  If you want to know something of the true stor...   imdb\n",
      "\n",
      "Saved to: ./Sentiment Analysis CSV/the_kingmaker_reviews_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "# Create individual movie sentiment files\n",
    "print(\"Creating individual movie sentiment analysis files...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dictionary of all review dataframes\n",
    "all_reviews_dict = {\n",
    "    'The Kingmaker': the_kingmaker_reviews_df,\n",
    "    'Praybeyt Benjamin 2': praybeyt_benjamin_2_reviews_df,\n",
    "    'Hello Love Goodbye': hello_love_goodbye_reviews_df,\n",
    "    'Hayop Ka': hayop_ka_reviews_df,\n",
    "    'Sunshine': sunshine_reviews_df,\n",
    "    'Quezon': quezon_reviews_df,\n",
    "    'Praybeyt Benjamin 1': praybeyt_benjamin_1_reviews_df,\n",
    "    'Mallari': mallari_reviews_df,\n",
    "    'Maid in Malacañang': maid_in_malacanang_reviews_df,\n",
    "    'Hows of Us': hows_of_us_reviews_df,\n",
    "    'Felix Manalo': felix_manalo_reviews_df\n",
    "}\n",
    "\n",
    "# Store all sentences dataframes\n",
    "all_sentences_dfs = {}\n",
    "total_sentences = 0\n",
    "\n",
    "# Process each movie\n",
    "for movie_name, reviews_df in all_reviews_dict.items():\n",
    "    if reviews_df is not None and not reviews_df.empty:\n",
    "        print(f\"\\nProcessing: {movie_name}\")\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences_df = split_reviews_into_sentences(reviews_df, movie_name)\n",
    "        all_sentences_dfs[movie_name] = sentences_df\n",
    "        \n",
    "        # Create filename\n",
    "        filename = movie_name.lower().replace(' ', '_').replace('ñ', 'n').replace('!', '').replace('.', '')\n",
    "        filename = f\"{filename}_reviews_sentiment.csv\"\n",
    "        filepath = os.path.join(sentiment_dir, filename)\n",
    "        \n",
    "        # Save individual movie file\n",
    "        sentences_df.to_csv(filepath, index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"  Original reviews: {reviews_df.shape[0]}\")\n",
    "        print(f\"  Sentences extracted: {sentences_df.shape[0]}\")\n",
    "        if reviews_df.shape[0] > 0:\n",
    "            avg_sentences = sentences_df.shape[0] / reviews_df.shape[0]\n",
    "            print(f\"  Avg sentences/review: {avg_sentences:.2f}\")\n",
    "        print(f\"  Saved to: {filepath}\")\n",
    "        \n",
    "        total_sentences += sentences_df.shape[0]\n",
    "    else:\n",
    "        print(f\"\\nSkipping {movie_name}: No reviews\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97436ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all review dataframes\n",
    "all_reviews_dict = {\n",
    "    'The Kingmaker': the_kingmaker_reviews_df,\n",
    "    'Praybeyt Benjamin 2': praybeyt_benjamin_2_reviews_df,\n",
    "    'Hello Love Goodbye': hello_love_goodbye_reviews_df,\n",
    "    'Hayop Ka': hayop_ka_reviews_df,\n",
    "    'Sunshine': sunshine_reviews_df,\n",
    "    'Quezon': quezon_reviews_df,\n",
    "    'Praybeyt Benjamin 1': praybeyt_benjamin_1_reviews_df,\n",
    "    'Mallari': mallari_reviews_df,\n",
    "    'Maid in Malacañang': maid_in_malacanang_reviews_df,\n",
    "    'Hows of Us': hows_of_us_reviews_df,\n",
    "    'Felix Manalo': felix_manalo_reviews_df\n",
    "}\n",
    "\n",
    "print(\"Creating sentiment analysis files...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store all sentences dataframes\n",
    "all_sentences_dfs = {}\n",
    "total_sentences = 0\n",
    "\n",
    "# Process each movie\n",
    "for movie_name, reviews_df in all_reviews_dict.items():\n",
    "    if reviews_df is not None and not reviews_df.empty:\n",
    "        \n",
    "        # Split into sentences\n",
    "        sentences_df = split_reviews_into_sentences(reviews_df, movie_name)\n",
    "        all_sentences_dfs[movie_name] = sentences_df\n",
    "        \n",
    "        # Create filename\n",
    "        filename = movie_name.lower().replace(' ', '_').replace('ñ', 'n').replace('!', '').replace('.', '')\n",
    "        filename = f\"{filename}_reviews_sentiment.csv\"\n",
    "        filepath = os.path.join(sentiment_dir, filename)\n",
    "        \n",
    "        # Save individual movie file\n",
    "        sentences_df.to_csv(filepath, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
