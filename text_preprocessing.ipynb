{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d89ce7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b65a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Kingmaker:\n",
      "imdb_the_kingmaker: (23, 5)\n",
      "ltbx_earliest_the_kingmaker: (816, 8)\n",
      "ltbx_latest_the_kingmaker: (636, 8)\n",
      "rt_the_kingmaker: (73, 7)\n",
      "ltbx_the_kingmaker: (1452, 8)\n",
      "\n",
      "Praybeyt Benjamin 2:\n",
      "imdb_praybeyt_benjamin_2: (6, 5)\n",
      "ltbx_earliest_praybeyt_benjamin_2: (144, 8)\n",
      "ltbx_latest_praybeyt_benjamin_2: (144, 8)\n",
      "ltbx_praybeyt_benjamin_2: (288, 8)\n",
      "\n",
      "Hello Love Goodbye:\n",
      "imdb_hello_love_goodbye: (44, 5)\n",
      "ltbx_earliest_hello_love_goodbye: (1548, 8)\n",
      "ltbx_latest_hello_love_goodbye: (1548, 8)\n",
      "rt_hello_love_goodbye: (10, 7)\n",
      "ltbx_hello_love_goodbye: (3096, 8)\n",
      "\n",
      "Hayop Ka:\n",
      "ltbx_earliest_hayop_ka: (456, 8)\n",
      "ltbx_latest_hayop_ka: (456, 8)\n",
      "rt_hayop_ka: (8, 7)\n",
      "ltbx_hayop_ka: (912, 8)\n",
      "\n",
      "Sunshine:\n",
      "imdb_sunshine: (15, 5)\n",
      "ltbx_earliest_sunshine: (3072, 8)\n",
      "ltbx_latest_sunshine: (3072, 8)\n",
      "ltbx_sunshine: (6144, 8)\n",
      "\n",
      "Quezon:\n",
      "imdb_quezon: (8, 5)\n",
      "ltbx_earliest_quezon: (1404, 8)\n",
      "ltbx_latest_quezon: (1404, 8)\n",
      "rt_quezon: (3, 9)\n",
      "ltbx_quezon: (2808, 8)\n",
      "\n",
      "Praybeyt Benjamin 1:\n",
      "ltbx_earliest_praybeyt_benjamin_1: (2400, 8)\n",
      "ltbx_latest_praybeyt_benjamin_1: (2400, 8)\n",
      "rt_praybeyt_benjamin_1: (19, 7)\n",
      "ltbx_praybeyt_benjamin_1: (4800, 8)\n",
      "\n",
      "Mallari:\n",
      "imdb_mallari: (18, 5)\n",
      "ltbx_earliest_mallari: (3000, 8)\n",
      "ltbx_latest_mallari: (3000, 8)\n",
      "rt_mallari: (9, 9)\n",
      "ltbx_mallari: (6000, 8)\n",
      "\n",
      "Maid in Malacañang:\n",
      "imdb_maid_in_malacanang: (1339, 5)\n",
      "ltbx_earliest_maid_in_malacanang: (336, 8)\n",
      "ltbx_latest_maid_in_malacanang: (336, 8)\n",
      "ltbx_maid_in_malacanang: (672, 8)\n",
      "\n",
      "Hows of Us:\n",
      "imdb_hows_of_us: (58, 5)\n",
      "ltbx_earliest_hows_of_us: (2220, 8)\n",
      "ltbx_latest_hows_of_us: (2220, 8)\n",
      "rt_hows_of_us: (8, 7)\n",
      "ltbx_hows_of_us: (4440, 8)\n",
      "\n",
      "Felix Manalo:\n",
      "ltbx_earliest_felix_manalo: (24, 8)\n",
      "ltbx_latest_felix_manalo: (40, 8)\n",
      "rt_felix_manalo: (3, 9)\n",
      "ltbx_felix_manalo: (64, 8)\n"
     ]
    }
   ],
   "source": [
    "# The Kingmaker - Complete\n",
    "imdb_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/imdb_the_kingmaker.csv\")\n",
    "ltbx_earliest_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/ltbx_earliest_the_kingmaker.csv\")\n",
    "ltbx_latest_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/ltbx_latest_the_kingmaker.csv\")\n",
    "rt_the_kingmaker = pd.read_csv(\"./Unclean Data/The Kingmaker/rt_the_kingmaker.csv\")\n",
    "\n",
    "ltbx_the_kingmaker = pd.concat([ltbx_earliest_the_kingmaker, ltbx_latest_the_kingmaker])\n",
    "\n",
    "print(\"The Kingmaker:\")\n",
    "print(f\"imdb_the_kingmaker: {imdb_the_kingmaker.shape}\")\n",
    "print(f\"ltbx_earliest_the_kingmaker: {ltbx_earliest_the_kingmaker.shape}\")\n",
    "print(f\"ltbx_latest_the_kingmaker: {ltbx_latest_the_kingmaker.shape}\")\n",
    "print(f\"rt_the_kingmaker: {rt_the_kingmaker.shape}\")\n",
    "print(f\"ltbx_the_kingmaker: {ltbx_the_kingmaker.shape}\")\n",
    "\n",
    "# Praybeyt Benjamin 2 - No RT\n",
    "imdb_praybeyt_benjamin_2 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 2/imdb_praybeyt_benjamin_2.csv\")\n",
    "ltbx_earliest_praybeyt_benjamin_2 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 2/ltbx_earliest_praybeyt_benjamin_2.csv\")\n",
    "ltbx_latest_praybeyt_benjamin_2 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 2/ltbx_latest_praybeyt_benjamin_2.csv\")\n",
    "\n",
    "ltbx_praybeyt_benjamin_2 = pd.concat([ltbx_earliest_praybeyt_benjamin_2, ltbx_latest_praybeyt_benjamin_2])\n",
    "\n",
    "print(\"\\nPraybeyt Benjamin 2:\")\n",
    "print(f\"imdb_praybeyt_benjamin_2: {imdb_praybeyt_benjamin_2.shape}\")\n",
    "print(f\"ltbx_earliest_praybeyt_benjamin_2: {ltbx_earliest_praybeyt_benjamin_2.shape}\")\n",
    "print(f\"ltbx_latest_praybeyt_benjamin_2: {ltbx_latest_praybeyt_benjamin_2.shape}\")\n",
    "print(f\"ltbx_praybeyt_benjamin_2: {ltbx_praybeyt_benjamin_2.shape}\")\n",
    "\n",
    "# Hello Love Goodbye - Complete\n",
    "imdb_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/imdb_hello_love_goodbye.csv\")\n",
    "ltbx_earliest_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/ltbx_earliest_hello_love_goodbye.csv\")\n",
    "ltbx_latest_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/ltbx_latest_hello_love_goodbye.csv\")\n",
    "rt_hello_love_goodbye = pd.read_csv(\"./Unclean Data/Hello Love Goodbye/rt_hello_love_goodbye.csv\")\n",
    "\n",
    "ltbx_hello_love_goodbye = pd.concat([ltbx_earliest_hello_love_goodbye, ltbx_latest_hello_love_goodbye])\n",
    "\n",
    "print(\"\\nHello Love Goodbye:\")\n",
    "print(f\"imdb_hello_love_goodbye: {imdb_hello_love_goodbye.shape}\")\n",
    "print(f\"ltbx_earliest_hello_love_goodbye: {ltbx_earliest_hello_love_goodbye.shape}\")\n",
    "print(f\"ltbx_latest_hello_love_goodbye: {ltbx_latest_hello_love_goodbye.shape}\")\n",
    "print(f\"rt_hello_love_goodbye: {rt_hello_love_goodbye.shape}\")\n",
    "print(f\"ltbx_hello_love_goodbye: {ltbx_hello_love_goodbye.shape}\")\n",
    "\n",
    "# Hayop Ka - Complete\n",
    "imdb_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/imdb_hayop_ka.csv\")\n",
    "ltbx_earliest_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/ltbx_earliest_hayop_ka.csv\")\n",
    "ltbx_latest_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/ltbx_latest_hayop_ka.csv\")\n",
    "rt_hayop_ka = pd.read_csv(\"./Unclean Data/Hayop Ka!/rt_hayop_ka.csv\")\n",
    "\n",
    "ltbx_hayop_ka = pd.concat([ltbx_earliest_hayop_ka, ltbx_latest_hayop_ka])\n",
    "\n",
    "print(\"\\nHayop Ka:\")\n",
    "print(f\"ltbx_earliest_hayop_ka: {ltbx_earliest_hayop_ka.shape}\")\n",
    "print(f\"ltbx_latest_hayop_ka: {ltbx_latest_hayop_ka.shape}\")\n",
    "print(f\"rt_hayop_ka: {rt_hayop_ka.shape}\")\n",
    "print(f\"ltbx_hayop_ka: {ltbx_hayop_ka.shape}\")\n",
    "\n",
    "# Sunshine - No RT\n",
    "imdb_sunshine = pd.read_csv(\"./Unclean Data/Sunshine/imdb_sunshine.csv\")\n",
    "ltbx_earliest_sunshine = pd.read_csv(\"./Unclean Data/Sunshine/ltbx_earliest_sunshine.csv\")\n",
    "ltbx_latest_sunshine = pd.read_csv(\"./Unclean Data/Sunshine/ltbx_latest_sunshine.csv\")\n",
    "\n",
    "ltbx_sunshine = pd.concat([ltbx_earliest_sunshine, ltbx_latest_sunshine])\n",
    "\n",
    "print(\"\\nSunshine:\")\n",
    "print(f\"imdb_sunshine: {imdb_sunshine.shape}\")\n",
    "print(f\"ltbx_earliest_sunshine: {ltbx_earliest_sunshine.shape}\")\n",
    "print(f\"ltbx_latest_sunshine: {ltbx_latest_sunshine.shape}\")\n",
    "print(f\"ltbx_sunshine: {ltbx_sunshine.shape}\")\n",
    "\n",
    "# Quezon - Complete\n",
    "imdb_quezon = pd.read_csv(\"./Unclean Data/Quezon/imdb_quezon.csv\")\n",
    "ltbx_earliest_quezon = pd.read_csv(\"./Unclean Data/Quezon/ltbx_earliest_quezon.csv\")\n",
    "ltbx_latest_quezon = pd.read_csv(\"./Unclean Data/Quezon/ltbx_latest_quezon.csv\")\n",
    "rt_quezon = pd.read_csv(\"./Unclean Data/Quezon/rt_quezon.csv\")\n",
    "\n",
    "ltbx_quezon = pd.concat([ltbx_earliest_quezon, ltbx_latest_quezon])\n",
    "\n",
    "print(\"\\nQuezon:\")\n",
    "print(f\"imdb_quezon: {imdb_quezon.shape}\")\n",
    "print(f\"ltbx_earliest_quezon: {ltbx_earliest_quezon.shape}\")\n",
    "print(f\"ltbx_latest_quezon: {ltbx_latest_quezon.shape}\")\n",
    "print(f\"rt_quezon: {rt_quezon.shape}\")\n",
    "print(f\"ltbx_quezon: {ltbx_quezon.shape}\")\n",
    "\n",
    "# Praybeyt Benjamin 1 - Complete\n",
    "imdb_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/imdb_praybeyt_benjamin_1.csv\")\n",
    "ltbx_earliest_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/ltbx_earliest_praybeyt_benjamin_1.csv\")\n",
    "ltbx_latest_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/ltbx_latest_praybeyt_benjamin_1.csv\")\n",
    "rt_praybeyt_benjamin_1 = pd.read_csv(\"./Unclean Data/Praybeyt Benjamin 1/rt_praybeyt_benjamin_1.csv\")\n",
    "\n",
    "ltbx_praybeyt_benjamin_1 = pd.concat([ltbx_earliest_praybeyt_benjamin_1, ltbx_latest_praybeyt_benjamin_1])\n",
    "\n",
    "print(\"\\nPraybeyt Benjamin 1:\")\n",
    "print(f\"ltbx_earliest_praybeyt_benjamin_1: {ltbx_earliest_praybeyt_benjamin_1.shape}\")\n",
    "print(f\"ltbx_latest_praybeyt_benjamin_1: {ltbx_latest_praybeyt_benjamin_1.shape}\")\n",
    "print(f\"rt_praybeyt_benjamin_1: {rt_praybeyt_benjamin_1.shape}\")\n",
    "print(f\"ltbx_praybeyt_benjamin_1: {ltbx_praybeyt_benjamin_1.shape}\")\n",
    "\n",
    "# Mallari - Complete\n",
    "imdb_mallari = pd.read_csv(\"./Unclean Data/Mallari/imdb_mallari.csv\")\n",
    "ltbx_earliest_mallari = pd.read_csv(\"./Unclean Data/Mallari/ltbx_earliest_mallari.csv\")\n",
    "ltbx_latest_mallari = pd.read_csv(\"./Unclean Data/Mallari/ltbx_latest_mallari.csv\")\n",
    "rt_mallari = pd.read_csv(\"./Unclean Data/Mallari/rt_mallari.csv\")\n",
    "\n",
    "ltbx_mallari = pd.concat([ltbx_earliest_mallari, ltbx_latest_mallari])\n",
    "\n",
    "print(\"\\nMallari:\")\n",
    "print(f\"imdb_mallari: {imdb_mallari.shape}\")\n",
    "print(f\"ltbx_earliest_mallari: {ltbx_earliest_mallari.shape}\")\n",
    "print(f\"ltbx_latest_mallari: {ltbx_latest_mallari.shape}\")\n",
    "print(f\"rt_mallari: {rt_mallari.shape}\")\n",
    "print(f\"ltbx_mallari: {ltbx_mallari.shape}\")\n",
    "\n",
    "# Maid in Malacanang - No RT\n",
    "imdb_maid_in_malacanang = pd.read_csv(\"./Unclean Data/Maid in Malacañang/imdb_maid_in_malacanang.csv\")\n",
    "ltbx_earliest_maid_in_malacanang = pd.read_csv(\"./Unclean Data/Maid in Malacañang/ltbx_earliest_maid_in_malacanang.csv\")\n",
    "ltbx_latest_maid_in_malacanang = pd.read_csv(\"./Unclean Data/Maid in Malacañang/ltbx_latest_maid_in_malacanang.csv\")\n",
    "\n",
    "ltbx_maid_in_malacanang = pd.concat([ltbx_earliest_maid_in_malacanang, ltbx_latest_maid_in_malacanang])\n",
    "\n",
    "print(\"\\nMaid in Malacañang:\")\n",
    "print(f\"imdb_maid_in_malacanang: {imdb_maid_in_malacanang.shape}\")\n",
    "print(f\"ltbx_earliest_maid_in_malacanang: {ltbx_earliest_maid_in_malacanang.shape}\")\n",
    "print(f\"ltbx_latest_maid_in_malacanang: {ltbx_latest_maid_in_malacanang.shape}\")\n",
    "print(f\"ltbx_maid_in_malacanang: {ltbx_maid_in_malacanang.shape}\")\n",
    "\n",
    "# Hows of Us - Complete\n",
    "imdb_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/imdb_hows_of_us.csv\")\n",
    "ltbx_earliest_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/ltbx_earliest_hows_of_us.csv\")\n",
    "ltbx_latest_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/ltbx_latest_hows_of_us.csv\")\n",
    "rt_hows_of_us = pd.read_csv(\"./Unclean Data/Hows of Us/rt_hows_of_us.csv\")\n",
    "\n",
    "ltbx_hows_of_us = pd.concat([ltbx_earliest_hows_of_us, ltbx_latest_hows_of_us])\n",
    "\n",
    "print(\"\\nHows of Us:\")\n",
    "print(f\"imdb_hows_of_us: {imdb_hows_of_us.shape}\")\n",
    "print(f\"ltbx_earliest_hows_of_us: {ltbx_earliest_hows_of_us.shape}\")\n",
    "print(f\"ltbx_latest_hows_of_us: {ltbx_latest_hows_of_us.shape}\")\n",
    "print(f\"rt_hows_of_us: {rt_hows_of_us.shape}\")\n",
    "print(f\"ltbx_hows_of_us: {ltbx_hows_of_us.shape}\")\n",
    "\n",
    "# Felix Manalo - Complete\n",
    "ltbx_earliest_felix_manalo = pd.read_csv(\"./Unclean Data/Felix Manalo/ltbx_earliest_felix_manalo.csv\")\n",
    "ltbx_latest_felix_manalo = pd.read_csv(\"./Unclean Data/Felix Manalo/ltbx_latest_felix_manalo.csv\")\n",
    "rt_felix_manalo = pd.read_csv(\"./Unclean Data/Felix Manalo/rt_felix_manalo.csv\")\n",
    "\n",
    "ltbx_felix_manalo = pd.concat([ltbx_earliest_felix_manalo, ltbx_latest_felix_manalo])\n",
    "\n",
    "print(\"\\nFelix Manalo:\")\n",
    "print(f\"ltbx_earliest_felix_manalo: {ltbx_earliest_felix_manalo.shape}\")\n",
    "print(f\"ltbx_latest_felix_manalo: {ltbx_latest_felix_manalo.shape}\")\n",
    "print(f\"rt_felix_manalo: {rt_felix_manalo.shape}\")\n",
    "print(f\"ltbx_felix_manalo: {ltbx_felix_manalo.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6081b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The Kingmaker ===\n",
      "imdb: 23 -> 23 (removed 0)\n",
      "ltbx: 1452 -> 1452 (removed 0)\n",
      "rt: 73 -> 73 (removed 0)\n",
      "\n",
      "=== Praybeyt Benjamin 2 ===\n",
      "imdb: 6 -> 6 (removed 0)\n",
      "ltbx: 288 -> 274 (removed 14)\n",
      "\n",
      "=== Hello Love Goodbye ===\n",
      "imdb: 44 -> 44 (removed 0)\n",
      "ltbx: 3096 -> 3095 (removed 1)\n",
      "rt: 10 -> 10 (removed 0)\n",
      "\n",
      "=== Hayop Ka ===\n",
      "imdb: 24 -> 24 (removed 0)\n",
      "ltbx: 912 -> 898 (removed 14)\n",
      "rt: 8 -> 8 (removed 0)\n",
      "\n",
      "=== Sunshine ===\n",
      "imdb: 15 -> 15 (removed 0)\n",
      "ltbx: 6144 -> 6140 (removed 4)\n",
      "\n",
      "=== Quezon ===\n",
      "imdb: 8 -> 8 (removed 0)\n",
      "ltbx: 2808 -> 2787 (removed 21)\n",
      "rt: 3 -> 3 (removed 0)\n",
      "\n",
      "=== Praybeyt Benjamin 1 ===\n",
      "imdb: 3 -> 3 (removed 0)\n",
      "ltbx: 4800 -> 605 (removed 4195)\n",
      "rt: 19 -> 19 (removed 0)\n",
      "\n",
      "=== Mallari ===\n",
      "imdb: 18 -> 18 (removed 0)\n",
      "ltbx: 6000 -> 5975 (removed 25)\n",
      "rt: 9 -> 9 (removed 0)\n",
      "\n",
      "=== Maid in Malacañang ===\n",
      "imdb: 1339 -> 1339 (removed 0)\n",
      "ltbx: 672 -> 668 (removed 4)\n",
      "\n",
      "=== Hows of Us ===\n",
      "imdb: 58 -> 58 (removed 0)\n",
      "ltbx: 4440 -> 4426 (removed 14)\n",
      "rt: 8 -> 8 (removed 0)\n",
      "\n",
      "================================================================================\n",
      "All dataframes cleaned successfully!\n",
      "Now checking for duplicate review text within each source...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Clean The Kingmaker\n",
    "print(\"=== The Kingmaker ===\")\n",
    "imdb_before = imdb_the_kingmaker.shape[0]\n",
    "ltbx_before = ltbx_the_kingmaker.shape[0]\n",
    "rt_before = rt_the_kingmaker.shape[0]\n",
    "\n",
    "imdb_the_kingmaker = imdb_the_kingmaker.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_the_kingmaker = ltbx_the_kingmaker.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_the_kingmaker = rt_the_kingmaker.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_the_kingmaker.shape[0]} (removed {imdb_before - imdb_the_kingmaker.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_the_kingmaker.shape[0]} (removed {ltbx_before - ltbx_the_kingmaker.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_the_kingmaker.shape[0]} (removed {rt_before - rt_the_kingmaker.shape[0]})\")\n",
    "\n",
    "# Clean Praybeyt Benjamin 2\n",
    "print(\"\\n=== Praybeyt Benjamin 2 ===\")\n",
    "imdb_before = imdb_praybeyt_benjamin_2.shape[0]\n",
    "ltbx_before = ltbx_praybeyt_benjamin_2.shape[0]\n",
    "\n",
    "imdb_praybeyt_benjamin_2 = imdb_praybeyt_benjamin_2.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_praybeyt_benjamin_2 = ltbx_praybeyt_benjamin_2.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_praybeyt_benjamin_2.shape[0]} (removed {imdb_before - imdb_praybeyt_benjamin_2.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_praybeyt_benjamin_2.shape[0]} (removed {ltbx_before - ltbx_praybeyt_benjamin_2.shape[0]})\")\n",
    "\n",
    "# Clean Hello Love Goodbye\n",
    "print(\"\\n=== Hello Love Goodbye ===\")\n",
    "imdb_before = imdb_hello_love_goodbye.shape[0]\n",
    "ltbx_before = ltbx_hello_love_goodbye.shape[0]\n",
    "rt_before = rt_hello_love_goodbye.shape[0]\n",
    "\n",
    "imdb_hello_love_goodbye = imdb_hello_love_goodbye.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_hello_love_goodbye = ltbx_hello_love_goodbye.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_hello_love_goodbye = rt_hello_love_goodbye.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_hello_love_goodbye.shape[0]} (removed {imdb_before - imdb_hello_love_goodbye.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_hello_love_goodbye.shape[0]} (removed {ltbx_before - ltbx_hello_love_goodbye.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_hello_love_goodbye.shape[0]} (removed {rt_before - rt_hello_love_goodbye.shape[0]})\")\n",
    "\n",
    "# Clean Hayop Ka\n",
    "print(\"\\n=== Hayop Ka ===\")\n",
    "imdb_before = imdb_hayop_ka.shape[0]\n",
    "ltbx_before = ltbx_hayop_ka.shape[0]\n",
    "rt_before = rt_hayop_ka.shape[0]\n",
    "\n",
    "imdb_hayop_ka = imdb_hayop_ka.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_hayop_ka = ltbx_hayop_ka.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_hayop_ka = rt_hayop_ka.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_hayop_ka.shape[0]} (removed {imdb_before - imdb_hayop_ka.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_hayop_ka.shape[0]} (removed {ltbx_before - ltbx_hayop_ka.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_hayop_ka.shape[0]} (removed {rt_before - rt_hayop_ka.shape[0]})\")\n",
    "\n",
    "# Clean Sunshine\n",
    "print(\"\\n=== Sunshine ===\")\n",
    "imdb_before = imdb_sunshine.shape[0]\n",
    "ltbx_before = ltbx_sunshine.shape[0]\n",
    "\n",
    "imdb_sunshine = imdb_sunshine.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_sunshine = ltbx_sunshine.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_sunshine.shape[0]} (removed {imdb_before - imdb_sunshine.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_sunshine.shape[0]} (removed {ltbx_before - ltbx_sunshine.shape[0]})\")\n",
    "\n",
    "# Clean Quezon\n",
    "print(\"\\n=== Quezon ===\")\n",
    "imdb_before = imdb_quezon.shape[0]\n",
    "ltbx_before = ltbx_quezon.shape[0]\n",
    "rt_before = rt_quezon.shape[0]\n",
    "\n",
    "imdb_quezon = imdb_quezon.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_quezon = ltbx_quezon.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_quezon = rt_quezon.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_quezon.shape[0]} (removed {imdb_before - imdb_quezon.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_quezon.shape[0]} (removed {ltbx_before - ltbx_quezon.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_quezon.shape[0]} (removed {rt_before - rt_quezon.shape[0]})\")\n",
    "\n",
    "# Clean Praybeyt Benjamin 1\n",
    "print(\"\\n=== Praybeyt Benjamin 1 ===\")\n",
    "imdb_before = imdb_praybeyt_benjamin_1.shape[0]\n",
    "ltbx_before = ltbx_praybeyt_benjamin_1.shape[0]\n",
    "rt_before = rt_praybeyt_benjamin_1.shape[0]\n",
    "\n",
    "imdb_praybeyt_benjamin_1 = imdb_praybeyt_benjamin_1.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_praybeyt_benjamin_1 = ltbx_praybeyt_benjamin_1.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_praybeyt_benjamin_1 = rt_praybeyt_benjamin_1.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_praybeyt_benjamin_1.shape[0]} (removed {imdb_before - imdb_praybeyt_benjamin_1.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_praybeyt_benjamin_1.shape[0]} (removed {ltbx_before - ltbx_praybeyt_benjamin_1.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_praybeyt_benjamin_1.shape[0]} (removed {rt_before - rt_praybeyt_benjamin_1.shape[0]})\")\n",
    "\n",
    "# Clean Mallari\n",
    "print(\"\\n=== Mallari ===\")\n",
    "imdb_before = imdb_mallari.shape[0]\n",
    "ltbx_before = ltbx_mallari.shape[0]\n",
    "rt_before = rt_mallari.shape[0]\n",
    "\n",
    "imdb_mallari = imdb_mallari.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_mallari = ltbx_mallari.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_mallari = rt_mallari.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_mallari.shape[0]} (removed {imdb_before - imdb_mallari.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_mallari.shape[0]} (removed {ltbx_before - ltbx_mallari.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_mallari.shape[0]} (removed {rt_before - rt_mallari.shape[0]})\")\n",
    "\n",
    "# Clean Maid in Malacañang\n",
    "print(\"\\n=== Maid in Malacañang ===\")\n",
    "imdb_before = imdb_maid_in_malacanang.shape[0]\n",
    "ltbx_before = ltbx_maid_in_malacanang.shape[0]\n",
    "\n",
    "imdb_maid_in_malacanang = imdb_maid_in_malacanang.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_maid_in_malacanang = ltbx_maid_in_malacanang.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_maid_in_malacanang.shape[0]} (removed {imdb_before - imdb_maid_in_malacanang.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_maid_in_malacanang.shape[0]} (removed {ltbx_before - ltbx_maid_in_malacanang.shape[0]})\")\n",
    "\n",
    "# Clean Hows of Us\n",
    "print(\"\\n=== Hows of Us ===\")\n",
    "imdb_before = imdb_hows_of_us.shape[0]\n",
    "ltbx_before = ltbx_hows_of_us.shape[0]\n",
    "rt_before = rt_hows_of_us.shape[0]\n",
    "\n",
    "imdb_hows_of_us = imdb_hows_of_us.drop_duplicates(subset=['Username', 'Date', 'Review Text'], keep='first')\n",
    "ltbx_hows_of_us = ltbx_hows_of_us.drop_duplicates(subset=['username', 'date', 'viewing_type', 'review_text'], keep='first')\n",
    "rt_hows_of_us = rt_hows_of_us.drop_duplicates(subset=['Name', 'Date', 'Review Content'], keep='first')\n",
    "\n",
    "print(f\"imdb: {imdb_before} -> {imdb_hows_of_us.shape[0]} (removed {imdb_before - imdb_hows_of_us.shape[0]})\")\n",
    "print(f\"ltbx: {ltbx_before} -> {ltbx_hows_of_us.shape[0]} (removed {ltbx_before - ltbx_hows_of_us.shape[0]})\")\n",
    "print(f\"rt: {rt_before} -> {rt_hows_of_us.shape[0]} (removed {rt_before - rt_hows_of_us.shape[0]})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All dataframes cleaned successfully!\")\n",
    "print(\"Now checking for duplicate review text within each source...\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9abc7e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>viewing_type</th>\n",
       "      <th>user_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rusluc</td>\n",
       "      <td>10.0</td>\n",
       "      <td>best documentary i watched in a while.. a very...</td>\n",
       "      <td>2025-12-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>watch</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  username  rating                                        review_text  \\\n",
       "0   rusluc    10.0  best documentary i watched in a while.. a very...   \n",
       "\n",
       "         date  likes  comments viewing_type  user_liked  \n",
       "0  2025-12-07      0         0        watch        True  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltbx_the_kingmaker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "116c7e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Filter</th>\n",
       "      <th>Review Content</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Source Filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Audience</td>\n",
       "      <td>Audience</td>\n",
       "      <td>A thoroughly eye-opening account of corruption...</td>\n",
       "      <td>C H</td>\n",
       "      <td>Mar 24</td>\n",
       "      <td>5.0/5</td>\n",
       "      <td>all-audience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type    Filter                                     Review Content Name  \\\n",
       "0  Audience  Audience  A thoroughly eye-opening account of corruption...  C H   \n",
       "\n",
       "     Date Rating Source Filter  \n",
       "0  Mar 24  5.0/5  all-audience  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_the_kingmaker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fab0f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VoNiTo101</td>\n",
       "      <td>May 6, 2022</td>\n",
       "      <td>This is the truth</td>\n",
       "      <td>This documentary perfectly captured the experi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Username         Date              Title  \\\n",
       "0  VoNiTo101  May 6, 2022  This is the truth   \n",
       "\n",
       "                                         Review Text  Rating  \n",
       "0  This documentary perfectly captured the experi...       9  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_the_kingmaker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "412d3c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['username', 'rating', 'review_text', 'date', 'likes', 'comments',\n",
       "       'viewing_type', 'user_liked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltbx_the_kingmaker.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f8f08fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'Filter', 'Review Content', 'Name', 'Date', 'Rating',\n",
       "       'Source Filter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_the_kingmaker.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cdc513a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Username', 'Date', 'Title', 'Review Text', 'Rating'], dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_the_kingmaker.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b93ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create combined review dataframe for a movie\n",
    "def create_combined_reviews_df(imdb_df, ltbx_df, rt_df):\n",
    "    \n",
    "    # Create empty list to store all reviews\n",
    "    all_reviews = []\n",
    "    \n",
    "    # Process IMDb reviews\n",
    "    if imdb_df is not None and not imdb_df.empty:\n",
    "        for _, row in imdb_df.iterrows():\n",
    "            # Create review text: title + review text\n",
    "            review_text = f\"{row['Title']}: {row['Review Text']}\" if pd.notna(row['Title']) and pd.notna(row['Review Text']) else row.get('Review Text', '')\n",
    "            \n",
    "            # Convert rating to percentage (IMDb uses 1-10 scale)\n",
    "            try:\n",
    "                # Handle different formats: \"7/10\", \"7.5/10\", \"7\", 7.5\n",
    "                if isinstance(row['Rating'], str):\n",
    "                    if '/' in row['Rating']:\n",
    "                        num, denom = row['Rating'].split('/')\n",
    "                        rating = (float(num) / float(denom)) * 100\n",
    "                    else:\n",
    "                        rating = float(row['Rating']) * 10  # Convert 0-10 to 0-100\n",
    "                else:\n",
    "                    rating = float(row['Rating']) * 10  # Convert 0-10 to 0-100\n",
    "            except:\n",
    "                rating = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                'username': row['Username'],\n",
    "                'date': row['Date'],\n",
    "                'review_text': review_text,\n",
    "                'rating': rating,\n",
    "                'source': 'imdb'\n",
    "            })\n",
    "    \n",
    "    # Process Letterboxd reviews\n",
    "    if ltbx_df is not None and not ltbx_df.empty:\n",
    "        for _, row in ltbx_df.iterrows():\n",
    "            # Convert rating to percentage (Letterboxd uses 0.5-5 scale)\n",
    "            try:\n",
    "                if pd.notna(row['rating']):\n",
    "                    # Handle different formats: 4.0, 4.5, \"4\", \"4.5\"\n",
    "                    rating = (float(row['rating']) / 5) * 100\n",
    "                else:\n",
    "                    rating = None\n",
    "            except:\n",
    "                rating = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                'username': row['username'],\n",
    "                'date': row['date'],\n",
    "                'review_text': row.get('review_text', ''),\n",
    "                'rating': rating,\n",
    "                'source': 'ltbx'\n",
    "            })\n",
    "    \n",
    "    # Process Rotten Tomatoes reviews\n",
    "    if rt_df is not None and not rt_df.empty:\n",
    "        for _, row in rt_df.iterrows():\n",
    "            # Convert rating to percentage\n",
    "            try:\n",
    "                if pd.notna(row['Rating']):\n",
    "                    # Handle different formats: \"4/5\", \"3.5/5\", \"Fresh\", \"Rotten\"\n",
    "                    if isinstance(row['Rating'], str):\n",
    "                        if '/' in row['Rating']:\n",
    "                            num, denom = row['Rating'].split('/')\n",
    "                            rating = (float(num) / float(denom)) * 100\n",
    "                        elif row['Rating'].lower() == 'fresh':\n",
    "                            rating = 100  # Fresh = positive = 100%\n",
    "                        elif row['Rating'].lower() == 'rotten':\n",
    "                            rating = 0    # Rotten = negative = 0%\n",
    "                        else:\n",
    "                            rating = None\n",
    "                    else:\n",
    "                        rating = float(row['Rating'])  # Assume already in percentage\n",
    "                else:\n",
    "                    rating = None\n",
    "            except:\n",
    "                rating = None\n",
    "            \n",
    "            all_reviews.append({\n",
    "                'username': row['Name'],\n",
    "                'date': row['Date'],\n",
    "                'review_text': row.get('Review Content', ''),\n",
    "                'rating': rating,\n",
    "                'source': 'rt'\n",
    "            })\n",
    "    \n",
    "    # Create dataframe\n",
    "    reviews_df = pd.DataFrame(all_reviews)\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    if 'date' in reviews_df.columns and not reviews_df.empty:\n",
    "        reviews_df['date'] = pd.to_datetime(reviews_df['date'], errors='coerce')\n",
    "    \n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b8027d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Review DataFrames Summary:\n",
      "================================================================================\n",
      "The Kingmaker: 1548 reviews\n",
      "  Sources: {'ltbx': 1452, 'rt': 73, 'imdb': 23}\n",
      "\n",
      "Praybeyt Benjamin 2: 280 reviews\n",
      "  Sources: {'ltbx': 274, 'imdb': 6}\n",
      "\n",
      "Hello Love Goodbye: 3149 reviews\n",
      "  Sources: {'ltbx': 3095, 'imdb': 44, 'rt': 10}\n",
      "\n",
      "Hayop Ka: 930 reviews\n",
      "  Sources: {'ltbx': 898, 'imdb': 24, 'rt': 8}\n",
      "\n",
      "Sunshine: 6155 reviews\n",
      "  Sources: {'ltbx': 6140, 'imdb': 15}\n",
      "\n",
      "Quezon: 2798 reviews\n",
      "  Sources: {'ltbx': 2787, 'imdb': 8, 'rt': 3}\n",
      "\n",
      "Praybeyt Benjamin 1: 627 reviews\n",
      "  Sources: {'ltbx': 605, 'rt': 19, 'imdb': 3}\n",
      "\n",
      "Mallari: 6002 reviews\n",
      "  Sources: {'ltbx': 5975, 'imdb': 18, 'rt': 9}\n",
      "\n",
      "Maid in Malacañang: 2007 reviews\n",
      "  Sources: {'imdb': 1339, 'ltbx': 668}\n",
      "\n",
      "Hows of Us: 4492 reviews\n",
      "  Sources: {'ltbx': 4426, 'imdb': 58, 'rt': 8}\n",
      "\n",
      "Felix Manalo: 67 reviews\n",
      "  Sources: {'ltbx': 64, 'rt': 3}\n"
     ]
    }
   ],
   "source": [
    "# Create combined review dataframes for all movies\n",
    "the_kingmaker_reviews_df = create_combined_reviews_df(imdb_the_kingmaker, ltbx_the_kingmaker, rt_the_kingmaker)\n",
    "praybeyt_benjamin_2_reviews_df = create_combined_reviews_df(imdb_praybeyt_benjamin_2, ltbx_praybeyt_benjamin_2, None)\n",
    "hello_love_goodbye_reviews_df = create_combined_reviews_df(imdb_hello_love_goodbye, ltbx_hello_love_goodbye, rt_hello_love_goodbye)\n",
    "hayop_ka_reviews_df = create_combined_reviews_df(imdb_hayop_ka, ltbx_hayop_ka, rt_hayop_ka)\n",
    "sunshine_reviews_df = create_combined_reviews_df(imdb_sunshine, ltbx_sunshine, None)\n",
    "quezon_reviews_df = create_combined_reviews_df(imdb_quezon, ltbx_quezon, rt_quezon)\n",
    "praybeyt_benjamin_1_reviews_df = create_combined_reviews_df(imdb_praybeyt_benjamin_1, ltbx_praybeyt_benjamin_1, rt_praybeyt_benjamin_1)\n",
    "mallari_reviews_df = create_combined_reviews_df(imdb_mallari, ltbx_mallari, rt_mallari)\n",
    "maid_in_malacanang_reviews_df = create_combined_reviews_df(imdb_maid_in_malacanang, ltbx_maid_in_malacanang, None)\n",
    "hows_of_us_reviews_df = create_combined_reviews_df(imdb_hows_of_us, ltbx_hows_of_us, rt_hows_of_us)\n",
    "felix_manalo_reviews_df = create_combined_reviews_df(None, ltbx_felix_manalo, rt_felix_manalo)\n",
    "\n",
    "# Print summary for each movie\n",
    "print(\"Combined Review DataFrames Summary:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"The Kingmaker: {the_kingmaker_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {the_kingmaker_reviews_df['source'].value_counts().to_dict() if not the_kingmaker_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nPraybeyt Benjamin 2: {praybeyt_benjamin_2_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {praybeyt_benjamin_2_reviews_df['source'].value_counts().to_dict() if not praybeyt_benjamin_2_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nHello Love Goodbye: {hello_love_goodbye_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {hello_love_goodbye_reviews_df['source'].value_counts().to_dict() if not hello_love_goodbye_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nHayop Ka: {hayop_ka_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {hayop_ka_reviews_df['source'].value_counts().to_dict() if not hayop_ka_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nSunshine: {sunshine_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {sunshine_reviews_df['source'].value_counts().to_dict() if not sunshine_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nQuezon: {quezon_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {quezon_reviews_df['source'].value_counts().to_dict() if not quezon_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nPraybeyt Benjamin 1: {praybeyt_benjamin_1_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {praybeyt_benjamin_1_reviews_df['source'].value_counts().to_dict() if not praybeyt_benjamin_1_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nMallari: {mallari_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {mallari_reviews_df['source'].value_counts().to_dict() if not mallari_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nMaid in Malacañang: {maid_in_malacanang_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {maid_in_malacanang_reviews_df['source'].value_counts().to_dict() if not maid_in_malacanang_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nHows of Us: {hows_of_us_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {hows_of_us_reviews_df['source'].value_counts().to_dict() if not hows_of_us_reviews_df.empty else {}}\")\n",
    "\n",
    "print(f\"\\nFelix Manalo: {felix_manalo_reviews_df.shape[0]} reviews\")\n",
    "print(f\"  Sources: {felix_manalo_reviews_df['source'].value_counts().to_dict() if not felix_manalo_reviews_df.empty else {}}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9572fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All review dataframes exported to CSV files:\n",
      "================================================================================\n",
      "1. The Kingmaker: ./Cleaned Data/the_kingmaker_reviews.csv\n",
      "2. Praybeyt Benjamin 2: ./Cleaned Data/praybeyt_benjamin_2_reviews.csv\n",
      "3. Hello Love Goodbye: ./Cleaned Data/hello_love_goodbye_reviews.csv\n",
      "4. Hayop Ka: ./Cleaned Data/hayop_ka_reviews.csv\n",
      "5. Sunshine: ./Cleaned Data/sunshine_reviews.csv\n",
      "6. Quezon: ./Cleaned Data/quezon_reviews.csv\n",
      "7. Praybeyt Benjamin 1: ./Cleaned Data/praybeyt_benjamin_1_reviews.csv\n",
      "8. Mallari: ./Cleaned Data/mallari_reviews.csv\n",
      "9. Maid in Malacañang: ./Cleaned Data/maid_in_malacanang_reviews.csv\n",
      "10. Hows of Us: ./Cleaned Data/hows_of_us_reviews.csv\n",
      "11. Felix Manalo: ./Cleaned Data/felix_manalo_reviews.csv\n",
      "\n",
      "================================================================================\n",
      "Files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create Cleaned Data directory if it doesn't exist\n",
    "cleaned_data_dir = \"./Cleaned Data\"\n",
    "if not os.path.exists(cleaned_data_dir):\n",
    "    os.makedirs(cleaned_data_dir)\n",
    "\n",
    "# Export each reviews dataframe to CSV (without _df in filename)\n",
    "the_kingmaker_reviews_df.to_csv(f\"{cleaned_data_dir}/the_kingmaker_reviews.csv\", index=False)\n",
    "praybeyt_benjamin_2_reviews_df.to_csv(f\"{cleaned_data_dir}/praybeyt_benjamin_2_reviews.csv\", index=False)\n",
    "hello_love_goodbye_reviews_df.to_csv(f\"{cleaned_data_dir}/hello_love_goodbye_reviews.csv\", index=False)\n",
    "hayop_ka_reviews_df.to_csv(f\"{cleaned_data_dir}/hayop_ka_reviews.csv\", index=False)\n",
    "sunshine_reviews_df.to_csv(f\"{cleaned_data_dir}/sunshine_reviews.csv\", index=False)\n",
    "quezon_reviews_df.to_csv(f\"{cleaned_data_dir}/quezon_reviews.csv\", index=False)\n",
    "praybeyt_benjamin_1_reviews_df.to_csv(f\"{cleaned_data_dir}/praybeyt_benjamin_1_reviews.csv\", index=False)\n",
    "mallari_reviews_df.to_csv(f\"{cleaned_data_dir}/mallari_reviews.csv\", index=False)\n",
    "maid_in_malacanang_reviews_df.to_csv(f\"{cleaned_data_dir}/maid_in_malacanang_reviews.csv\", index=False)\n",
    "hows_of_us_reviews_df.to_csv(f\"{cleaned_data_dir}/hows_of_us_reviews.csv\", index=False)\n",
    "felix_manalo_reviews_df.to_csv(f\"{cleaned_data_dir}/felix_manalo_reviews.csv\", index=False)\n",
    "\n",
    "print(\"All review dataframes exported to CSV files:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"1. The Kingmaker: {cleaned_data_dir}/the_kingmaker_reviews.csv\")\n",
    "print(f\"2. Praybeyt Benjamin 2: {cleaned_data_dir}/praybeyt_benjamin_2_reviews.csv\")\n",
    "print(f\"3. Hello Love Goodbye: {cleaned_data_dir}/hello_love_goodbye_reviews.csv\")\n",
    "print(f\"4. Hayop Ka: {cleaned_data_dir}/hayop_ka_reviews.csv\")\n",
    "print(f\"5. Sunshine: {cleaned_data_dir}/sunshine_reviews.csv\")\n",
    "print(f\"6. Quezon: {cleaned_data_dir}/quezon_reviews.csv\")\n",
    "print(f\"7. Praybeyt Benjamin 1: {cleaned_data_dir}/praybeyt_benjamin_1_reviews.csv\")\n",
    "print(f\"8. Mallari: {cleaned_data_dir}/mallari_reviews.csv\")\n",
    "print(f\"9. Maid in Malacañang: {cleaned_data_dir}/maid_in_malacanang_reviews.csv\")\n",
    "print(f\"10. Hows of Us: {cleaned_data_dir}/hows_of_us_reviews.csv\")\n",
    "print(f\"11. Felix Manalo: {cleaned_data_dir}/felix_manalo_reviews.csv\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Files saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22955fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
